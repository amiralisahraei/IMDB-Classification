# -*- coding: utf-8 -*-
"""Chollet_text_IMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tTBSxUeprP6TJ_Sk8qUK6nPxHrQLn1Ta

One-hot encoding for words
"""

import numpy as np

samples = [ 'the cat sat on the mat.','the dog ate my homework.']

token_index = {}
for sample in samples:
  for word in sample.split():
    if word not in token_index:
      token_index[word] = len(token_index)+1

token_index

max_length = 7
results = np.zeros(shape=(len(samples),max_length, max(token_index.values()) + 1))
max_length , max(token_index.values())

results

for i, sample in enumerate(samples):
  for j, word in list(enumerate(sample.split()))[:max_length]:
      index = token_index.get(word)
      results[i, j, index] = 1.

results

"""One-hot encoding for characters"""

import numpy as np
import string


samples = [ '001e cat sat on the mat.','the dog ate my homework.']
characters = string.printable
token_index = dict(zip(characters,range(1,len(characters)+1)))
results = np.zeros(shape=(len(samples),24, max(token_index.values()) + 1))
for i, sample in enumerate(samples):
  for j,character in enumerate(sample):
     index = token_index.get(character)
     results[i,j,index] = 1.

d=zip(characters,range(1,len(characters)+1))
list(d)

results

"""Listing 6.3 Using Keras for word-level one-hot encoding"""

from keras.preprocessing.text import Tokenizer

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(samples)

sequences = tokenizer.texts_to_sequences(samples)

one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')

word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens.')

word_index

sequences

"""Listing 6.4 Word-level one-hot encoding with hashing trick (toy example)"""

import numpy as np

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

dimensionality = 1000
max_length = 10

results = np.zeros((len(samples), max_length, dimensionality))

for i, sample in enumerate(samples):
    for j, word in list(enumerate(sample.split()))[:max_length]:
        index = abs(hash(word)) % dimensionality
        results[i, j, index] = 1.

(results[0][2] == 1).sum()

hash('ali')

"""IMDB"""

from keras.datasets import imdb
from keras import preprocessing

max_features = 10000
maxlen = 20

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

len(x_train[0])

x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)

np.array(x_train).shape , np.array(x_test).shape

from keras.models import Sequential
from keras.layers import Dense,Flatten,Embedding

model = Sequential()
model.add(Embedding( 10000, 8, input_length=maxlen))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile( optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])

history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

test_1 = x_test[0]
test_data = test_1.reshape(1,20)
result = model.predict(test_data)
result = np.round(result,3)
result , y_test[0]

import numpy as np

y = np.array([[[2,5],[3,7]], [[4,9],[5,10]]])
out=y.flatten()
# out , out.shape, y.shape
x_train.shape

Pretrainde emabedding

import os, shutil
import cv2
import glob
import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

!unzip "/content/glove.6B.zip"

data = pd.read_csv('/content/IMDB Dataset.csv')
# data.head()
texts = list(data['review'])
sentiment = list(data['sentiment'])
labels = []
for i,label in enumerate(sentiment):
  if label == 'positive':
     labels.insert(i,1)
  else:
     labels.insert(i,0)

labels = np.array(labels)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

maxlen = 100
training_samples = 200
validation_samples = 10000
max_words = 10000

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
data = pad_sequences(sequences,maxlen=maxlen)
data.shape[0] , labels.shape

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

x_train = data[:training_samples]
y_train = labels[:training_samples]
x_val = data[training_samples:training_samples+validation_samples]
y_val = labels[training_samples:training_samples+validation_samples]

reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))

reverse_word_map

sentence = {}
for i, word in reverse_word_map.items():
    if i in x_val[0]:
      sentence[i] = word

y_val[0]

"""Use Glove"""

!wget http://nlp.stanford.edu/data/glove.6B.zip

embeddings_index = {}
f = open(os.path.join('/content/glove.6B.100d.txt'))
for line in f:
  values = line.split()
  word = values[0]
  coefs = np.asarray(values[1:], dtype='float32')
  embeddings_index[word] = coefs
f.close()
print('Found %s word vectors.' % len(embeddings_index))

embedding_dim = 100

embedding_matrix = np.zeros((max_words,embedding_dim))

for word, i in word_index.items():
  if i < max_words:
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
      embedding_matrix[i] = embedding_vector

len(embedding_matrix)

from keras.models import Sequential
from keras.layers import Dense,Flatten,Embedding

model = Sequential()
model.add(Embedding( max_words, 100, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False

model.compile( loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])

history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))

import matplotlib.pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()

"""The Recurent Neural Network"""

from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense,LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D
from keras.datasets import imdb
from keras.preprocessing import sequence

max_features = 10000
maxlen = 500
batch_size = 32

(input_train, y_train),(input_test, y_test) = imdb.load_data(num_words=max_features)

input_train = sequence.pad_sequences(input_train, maxlen=maxlen)
input_test = sequence.pad_sequences(input_test, maxlen=maxlen)

model = Sequential()
model.add(Embedding(10000,32))
model.add(SimpleRNN(32))
model.add(Dense(1 , activation='sigmoid'))

model.compile( loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])

history = model.fit( input_train, y_train, batch_size=32, epochs=10, validation_split=0.2)

import matplotlib.pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()

"""LSTM"""

model = Sequential()
model.add(Embedding(10000,32))
model.add(LSTM(32))
model.add(Dense(1 , activation='sigmoid'))

model.compile( loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])

history = model.fit( input_train, y_train, batch_size=32, epochs=10, validation_split=0.2)

import matplotlib.pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()

"""Conv1D on IMDB"""

model = Sequential()
model.add(Embedding(max_features,128))
model.add(Conv1D(32,7, activation='relu'))
model.add(MaxPooling1D(5))
model.add(Conv1D(32,7, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(1 , activation='sigmoid'))

model.summary()

model.compile( loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])

history = model.fit( input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

import matplotlib.pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()